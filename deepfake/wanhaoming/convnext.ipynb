{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, txt_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            txt_file (string): Path to the txt file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(txt_file, header=0, names=['img_name', 'target'])\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        img_name = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = int(self.annotations.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_file = \"E:/DataSet/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/phase1/trainset_label.txt\"\n",
    "train_img_dir = \"E:/DataSet/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/phase1/trainset\"\n",
    "\n",
    "val_txt_file = \"E:/DataSet/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/phase1/valset_label.txt\"\n",
    "val_img_dir = \"E:/DataSet/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/waitan2024_deepfake_challenge__赛道1对外发布数据集_phase1/phase1/valset\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(txt_file=train_txt_file, img_dir=train_img_dir, transform=transform)\n",
    "val_dataset = ImageDataset(txt_file=val_txt_file, img_dir=val_img_dir, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "original code from facebook research:\n",
    "https://github.com/facebookresearch/ConvNeXt\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape), requires_grad=True)\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise ValueError(f\"not support data format '{self.data_format}'\")\n",
    "        self.normalized_shape = (normalized_shape,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            # [batch_size, channels, height, width]\n",
    "            mean = x.mean(1, keepdim=True)\n",
    "            var = (x - mean).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_rate (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_rate=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim)  # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6, data_format=\"channels_last\")\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)  # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim,)),\n",
    "                                  requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_rate) if drop_rate > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1)  # [N, C, H, W] -> [N, H, W, C]\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2)  # [N, H, W, C] -> [N, C, H, W]\n",
    "\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans: int = 3, num_classes: int = 1000, depths: list = None,\n",
    "                 dims: list = None, drop_path_rate: float = 0., layer_scale_init_value: float = 1e-6,\n",
    "                 head_init_scale: float = 1.):\n",
    "        super().__init__()\n",
    "        self.downsample_layers = nn.ModuleList()  # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "                             LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\"))\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        # 对应stage2-stage4前的3个downsample\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                                             nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2))\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList()  # 4 feature resolution stages, each consisting of multiple blocks\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        # 构建每个stage中堆叠的block\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_rate=dp_rates[cur + j], layer_scale_init_value=layer_scale_init_value)\n",
    "                  for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)  # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.2)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "\n",
    "        return self.norm(x.mean([-2, -1]))  # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def convnext_tiny(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3],\n",
    "                     dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_small(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[96, 192, 384, 768],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_base(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[128, 256, 512, 1024],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_large(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[192, 384, 768, 1536],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "def convnext_xlarge(num_classes: int):\n",
    "    # https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3],\n",
    "                     dims=[256, 512, 1024, 2048],\n",
    "                     num_classes=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = convnext_base(100)\n",
    "model = model.cuda()\n",
    "x = torch.randn(4, 3, 512, 512).cuda()\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 load model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "\n",
    "# 加载权重，删除需要微调的权重\n",
    "device=\"cuda:0\"\n",
    "device = \"cuda:0\"\n",
    "model = model = convnext_base(100).to(device)\n",
    "\n",
    "\n",
    "# 设置学习率、优化器\n",
    "lr = 0.001\n",
    "lrf = 0.01\n",
    "epochs = 5\n",
    "pg = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(pg, lr=lr, momentum=0.9, weight_decay=5E-5)\n",
    "\n",
    "lf = lambda x: (1 + math.cos(x * math.pi / epochs) / 2) * (1 - lrf) + lrf  # cosine\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 如果训练则设置为True\n",
    "train_ = True\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, epoch):\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        accu_loss += loss\n",
    "\n",
    "        data_loader.desc = \"[valid epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    accu_loss = torch.zeros(1).to(device)  # 累计损失\n",
    "    accu_num = torch.zeros(1).to(device)   # 累计预测正确的样本数\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    sample_num = 0\n",
    "    data_loader = tqdm(data_loader, file=sys.stdout)\n",
    "    for step, data in enumerate(data_loader):\n",
    "        images, labels = data\n",
    "        sample_num += images.shape[0]\n",
    "\n",
    "        pred = model(images.to(device))\n",
    "        pred_classes = torch.max(pred, dim=1)[1]\n",
    "        accu_num += torch.eq(pred_classes, labels.to(device)).sum()\n",
    "\n",
    "        loss = loss_function(pred, labels.to(device))\n",
    "        loss.backward()\n",
    "        accu_loss += loss.detach()\n",
    "\n",
    "        data_loader.desc = \"[train epoch {}] loss: {:.3f}, acc: {:.3f}\".format(epoch,\n",
    "                                                                               accu_loss.item() / (step + 1),\n",
    "                                                                               accu_num.item() / sample_num)\n",
    "\n",
    "        if not torch.isfinite(loss):\n",
    "            print('WARNING: non-finite loss, ending training ', loss)\n",
    "            sys.exit(1)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return accu_loss.item() / (step + 1), accu_num.item() / sample_num\n",
    "\n",
    "if train_:\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        train_loss, train_acc = train_one_epoch(model=model,\n",
    "                                                optimizer=optimizer,\n",
    "                                                data_loader=train_dataloader,\n",
    "                                                device=device,\n",
    "                                                epoch=epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # validate\n",
    "        val_loss, val_acc = evaluate(model=model,\n",
    "                                        data_loader=val_dataloader,\n",
    "                                        device=device,\n",
    "                                        epoch=epoch)\n",
    "        torch.save(model.state_dict(), \"./weights/model-{}.pth\".format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
